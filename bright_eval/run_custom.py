#!/usr/bin/env python3
"""
BRIGHT Benchmark è‡ªå®šä¹‰æ¨¡å‹è¯„ä¼°è„šæœ¬
====================================

ä¸€é”®è¿è¡Œæ‰€æœ‰ä»»åŠ¡å¹¶ç”Ÿæˆå¯æäº¤çš„ç»“æœï¼

ä½¿ç”¨æ–¹æ³•:
    # è¯„ä¼°å•ä¸ªä»»åŠ¡
    python run_custom.py --task biology

    # è¯„ä¼°æ‰€æœ‰ä»»åŠ¡
    python run_custom.py --all
    
    # è¯„ä¼°æ‰€æœ‰ä»»åŠ¡ï¼ˆé•¿æ–‡æ¡£è®¾ç½®ï¼‰
    python run_custom.py --all --long_context
"""

import os
import json
import argparse
from datetime import datetime
from tqdm import tqdm
from datasets import load_dataset
import pytrec_eval

# å¯¼å…¥è‡ªå®šä¹‰æ£€ç´¢å™¨
from custom_retriever import retrieval_custom


def calculate_retrieval_metrics(results, qrels, k_values=[1, 5, 10, 25, 50, 100]):
    """
    è®¡ç®—æ£€ç´¢æŒ‡æ ‡ (ä» BRIGHT retrievers.py å¤åˆ¶)
    """
    ndcg = {}
    _map = {}
    recall = {}
    precision = {}
    mrr = {"MRR": 0}

    for k in k_values:
        ndcg[f"NDCG@{k}"] = 0.0
        _map[f"MAP@{k}"] = 0.0
        recall[f"Recall@{k}"] = 0.0
        precision[f"P@{k}"] = 0.0

    map_string = "map_cut." + ",".join([str(k) for k in k_values])
    ndcg_string = "ndcg_cut." + ",".join([str(k) for k in k_values])
    recall_string = "recall." + ",".join([str(k) for k in k_values])
    precision_string = "P." + ",".join([str(k) for k in k_values])

    evaluator = pytrec_eval.RelevanceEvaluator(qrels,
                                               {map_string, ndcg_string, recall_string, precision_string, "recip_rank"})
    scores = evaluator.evaluate(results)

    for query_id in scores.keys():
        for k in k_values:
            ndcg[f"NDCG@{k}"] += scores[query_id]["ndcg_cut_" + str(k)]
            _map[f"MAP@{k}"] += scores[query_id]["map_cut_" + str(k)]
            recall[f"Recall@{k}"] += scores[query_id]["recall_" + str(k)]
            precision[f"P@{k}"] += scores[query_id]["P_" + str(k)]
        mrr["MRR"] += scores[query_id]["recip_rank"]

    for k in k_values:
        ndcg[f"NDCG@{k}"] = round(ndcg[f"NDCG@{k}"] / len(scores), 5)
        _map[f"MAP@{k}"] = round(_map[f"MAP@{k}"] / len(scores), 5)
        recall[f"Recall@{k}"] = round(recall[f"Recall@{k}"] / len(scores), 5)
        precision[f"P@{k}"] = round(precision[f"P@{k}"] / len(scores), 5)
    mrr["MRR"] = round(mrr["MRR"] / len(scores), 5)

    output = {**ndcg, **_map, **recall, **precision, **mrr}
    print(output)
    return output


# æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨
ALL_TASKS = [
    'biology', 'earth_science', 'economics', 'psychology', 'robotics',
    'stackoverflow', 'sustainable_living', 'leetcode', 'pony', 'aops',
    'theoremqa_questions', 'theoremqa_theorems'
]

# ç®€åŒ–çš„ä»»åŠ¡åˆ—è¡¨ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
QUICK_TASKS = ['biology', 'economics', 'leetcode']


def run_single_task(
    task: str,
    long_context: bool = False,
    cache_dir: str = 'cache',
    output_dir: str = 'outputs_custom',
    debug: bool = False
):
    """è¿è¡Œå•ä¸ªä»»åŠ¡è¯„ä¼°"""
    
    print(f"\n{'='*60}")
    print(f"è¯„ä¼°ä»»åŠ¡: {task}")
    print(f"é•¿æ–‡æ¡£æ¨¡å¼: {long_context}")
    print(f"{'='*60}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    task_output_dir = os.path.join(output_dir, f"{task}_custom_long_{long_context}")
    os.makedirs(task_output_dir, exist_ok=True)
    
    score_file = os.path.join(task_output_dir, 'score.json')
    result_file = os.path.join(task_output_dir, 'results.json')
    
    # åŠ è½½æ•°æ®
    print(f"åŠ è½½æ•°æ®é›†...")
    examples = load_dataset('xlangai/bright', 'examples', cache_dir=cache_dir)[task]
    
    if long_context:
        doc_pairs = load_dataset('xlangai/bright', 'long_documents', cache_dir=cache_dir)[task]
    else:
        doc_pairs = load_dataset('xlangai/bright', 'documents', cache_dir=cache_dir)[task]
    
    # å‡†å¤‡æ•°æ®
    documents = [dp['content'] for dp in doc_pairs]
    doc_ids = [dp['id'] for dp in doc_pairs]
    
    queries = [e['query'] for e in examples]
    query_ids = [e['id'] for e in examples]
    excluded_ids = {e['id']: e['excluded_ids'] for e in examples}
    
    if debug:
        documents = documents[:50]
        doc_ids = doc_ids[:50]
        queries = queries[:10]
        query_ids = query_ids[:10]
        excluded_ids = {qid: excluded_ids[qid] for qid in query_ids}
    
    print(f"æŸ¥è¯¢æ•°: {len(queries)}")
    print(f"æ–‡æ¡£æ•°: {len(documents)}")
    
    # è¿è¡Œæ£€ç´¢
    if not os.path.exists(score_file):
        scores = retrieval_custom(
            queries=queries,
            query_ids=query_ids,
            documents=documents,
            doc_ids=doc_ids,
            excluded_ids=excluded_ids,
            task=task,
            cache_dir=cache_dir,
            long_context=long_context,
            use_reranker=True,
            top_k=100
        )
        
        with open(score_file, 'w') as f:
            json.dump(scores, f, indent=2)
        print(f"åˆ†æ•°å·²ä¿å­˜: {score_file}")
    else:
        with open(score_file) as f:
            scores = json.load(f)
        print(f"ä½¿ç”¨ç¼“å­˜çš„åˆ†æ•°: {score_file}")
    
    # æ„å»º ground truth
    key = 'gold_ids_long' if long_context else 'gold_ids'
    ground_truth = {}
    for e in examples:
        if debug and e['id'] not in query_ids:
            continue
        ground_truth[e['id']] = {gid: 1 for gid in e[key]}
    
    # è®¡ç®—æŒ‡æ ‡
    print(f"\nè®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    results = calculate_retrieval_metrics(results=scores, qrels=ground_truth)
    
    with open(result_file, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"ç»“æœå·²ä¿å­˜: {result_file}")
    
    return {
        'task': task,
        'ndcg@10': results.get('NDCG@10', 0),
        'recall@1': results.get('Recall@1', 0),
        'recall@10': results.get('Recall@10', 0),
        'mrr': results.get('MRR', 0)
    }


def run_all_tasks(
    long_context: bool = False,
    cache_dir: str = 'cache',
    output_dir: str = 'outputs_custom',
    debug: bool = False,
    quick: bool = False
):
    """è¿è¡Œæ‰€æœ‰ä»»åŠ¡è¯„ä¼°"""
    
    tasks = QUICK_TASKS if quick else ALL_TASKS
    all_results = []
    
    print(f"\n{'#'*60}")
    print(f"# BRIGHT Benchmark å…¨é‡è¯„ä¼°")
    print(f"# ä»»åŠ¡æ•°: {len(tasks)}")
    print(f"# é•¿æ–‡æ¡£æ¨¡å¼: {long_context}")
    print(f"# æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'#'*60}")
    
    for i, task in enumerate(tasks, 1):
        print(f"\n[{i}/{len(tasks)}] ", end="")
        try:
            result = run_single_task(
                task=task,
                long_context=long_context,
                cache_dir=cache_dir,
                output_dir=output_dir,
                debug=debug
            )
            all_results.append(result)
        except Exception as e:
            print(f"ä»»åŠ¡ {task} å¤±è´¥: {e}")
            all_results.append({
                'task': task,
                'ndcg@10': 0,
                'recall@1': 0,
                'recall@10': 0,
                'mrr': 0,
                'error': str(e)
            })
    
    # æ±‡æ€»ç»“æœ
    summary = generate_summary(all_results, long_context, output_dir)
    
    return all_results, summary


def generate_summary(results: list, long_context: bool, output_dir: str):
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    
    # è®¡ç®—å¹³å‡åˆ†
    valid_results = [r for r in results if 'error' not in r]
    
    if not valid_results:
        print("æ²¡æœ‰æœ‰æ•ˆç»“æœï¼")
        return None
    
    avg_ndcg = sum(r['ndcg@10'] for r in valid_results) / len(valid_results)
    avg_recall1 = sum(r['recall@1'] for r in valid_results) / len(valid_results)
    avg_recall10 = sum(r['recall@10'] for r in valid_results) / len(valid_results)
    avg_mrr = sum(r['mrr'] for r in valid_results) / len(valid_results)
    
    summary = {
        'timestamp': datetime.now().isoformat(),
        'long_context': long_context,
        'num_tasks': len(valid_results),
        'average_scores': {
            'nDCG@10': round(avg_ndcg * 100, 2),  # è½¬ä¸ºç™¾åˆ†æ¯”
            'Recall@1': round(avg_recall1 * 100, 2),
            'Recall@10': round(avg_recall10 * 100, 2),
            'MRR': round(avg_mrr * 100, 2)
        },
        'per_task_scores': {r['task']: round(r['ndcg@10'] * 100, 2) for r in valid_results}
    }
    
    # ä¿å­˜æ±‡æ€»
    summary_file = os.path.join(output_dir, f'summary_long_{long_context}.json')
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2)
    
    # æ‰“å°æŠ¥å‘Š
    print(f"\n")
    print(f"{'='*60}")
    print(f"ğŸ“Š BRIGHT Benchmark è¯„ä¼°ç»“æœæ±‡æ€»")
    print(f"{'='*60}")
    print(f"è¯„ä¼°æ—¶é—´: {summary['timestamp']}")
    print(f"é•¿æ–‡æ¡£æ¨¡å¼: {long_context}")
    print(f"æœ‰æ•ˆä»»åŠ¡æ•°: {len(valid_results)}")
    print(f"")
    print(f"{'â”€'*60}")
    print(f"å¹³å‡åˆ†æ•°:")
    print(f"  â€¢ nDCG@10:   {summary['average_scores']['nDCG@10']:.2f}")
    print(f"  â€¢ Recall@1:  {summary['average_scores']['Recall@1']:.2f}")
    print(f"  â€¢ Recall@10: {summary['average_scores']['Recall@10']:.2f}")
    print(f"  â€¢ MRR:       {summary['average_scores']['MRR']:.2f}")
    print(f"")
    print(f"{'â”€'*60}")
    print(f"å„ä»»åŠ¡ nDCG@10 åˆ†æ•°:")
    for task, score in summary['per_task_scores'].items():
        print(f"  â€¢ {task:25s}: {score:.2f}")
    print(f"{'='*60}")
    print(f"")
    print(f"ğŸ“ æ±‡æ€»æ–‡ä»¶: {summary_file}")
    print(f"")
    print(f"{'â”€'*60}")
    print(f"ğŸ“§ æäº¤è¯´æ˜:")
    print(f"   å°†ç»“æœå‘é€è‡³: suhongjin96@gmail.com")
    print(f"   ä¸»è¦æŒ‡æ ‡: å¹³å‡ nDCG@10 = {summary['average_scores']['nDCG@10']:.2f}")
    print(f"{'='*60}")
    
    return summary


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='BRIGHT Benchmark è‡ªå®šä¹‰æ¨¡å‹è¯„ä¼°')
    parser.add_argument('--task', type=str, default=None,
                        help='å•ä¸ªä»»åŠ¡åç§°')
    parser.add_argument('--all', action='store_true',
                        help='è¯„ä¼°æ‰€æœ‰ä»»åŠ¡')
    parser.add_argument('--quick', action='store_true',
                        help='å¿«é€Ÿæµ‹è¯•ï¼ˆä»…3ä¸ªä»»åŠ¡ï¼‰')
    parser.add_argument('--long_context', action='store_true',
                        help='ä½¿ç”¨é•¿æ–‡æ¡£è®¾ç½®')
    parser.add_argument('--cache_dir', type=str, default='cache',
                        help='æ•°æ®ç¼“å­˜ç›®å½•')
    parser.add_argument('--output_dir', type=str, default='outputs_custom',
                        help='ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--debug', action='store_true',
                        help='è°ƒè¯•æ¨¡å¼ï¼ˆä½¿ç”¨å°‘é‡æ•°æ®ï¼‰')
    
    args = parser.parse_args()
    
    if args.all or args.quick:
        run_all_tasks(
            long_context=args.long_context,
            cache_dir=args.cache_dir,
            output_dir=args.output_dir,
            debug=args.debug,
            quick=args.quick
        )
    elif args.task:
        run_single_task(
            task=args.task,
            long_context=args.long_context,
            cache_dir=args.cache_dir,
            output_dir=args.output_dir,
            debug=args.debug
        )
    else:
        print("è¯·æŒ‡å®š --task <ä»»åŠ¡å> æˆ– --all æ¥è¿è¡Œè¯„ä¼°")
        print(f"å¯ç”¨ä»»åŠ¡: {', '.join(ALL_TASKS)}")
