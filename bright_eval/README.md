# BRIGHT Benchmark è¯„ä¼°ä¸æäº¤æŒ‡å—

æœ¬ç›®å½•åŒ…å«å‘ [BRIGHT Benchmark](https://brightbenchmark.github.io/) æäº¤çš„å®Œæ•´ä»£ç å’Œæ–‡æ¡£ã€‚

---

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [å®Œæ•´æµç¨‹å›¾](#-å®Œæ•´æµç¨‹å›¾)
- [æ•°æ®æ ¼å¼è¯´æ˜](#-æ•°æ®æ ¼å¼è¯´æ˜)
- [å¦‚ä½•æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹](#-å¦‚ä½•æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹)
- [è¯„ä¼°æŒ‡æ ‡è¯¦è§£](#-è¯„ä¼°æŒ‡æ ‡è¯¦è§£)
- [ç”Ÿæˆæäº¤æŠ¥å‘Š](#-ç”Ÿæˆæäº¤æŠ¥å‘Š)
- [æäº¤æ–¹æ³•](#-æäº¤æ–¹æ³•)
- [å¸¸è§é—®é¢˜](#-å¸¸è§é—®é¢˜)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
cd /Users/changhao/xhs_paper/PrincipleReasonRank

# åˆ›å»ºç¯å¢ƒ (æ¨è)
conda create -n principlerank python=3.10 -y
conda activate principlerank

# å®‰è£…ä¾èµ–
pip install datasets pytrec_eval rank_bm25 tqdm numpy transformers torch vllm
```

### 2. è¿è¡Œ BM25 åŸºçº¿

```bash
cd bright_eval

# å¿«é€Ÿæµ‹è¯• (3ä¸ªä»»åŠ¡)
python bright_evaluator.py --method bm25 --quick

# å…¨é‡è¯„ä¼° (12ä¸ªä»»åŠ¡)
python bright_evaluator.py --method bm25 --all --generate_report
```

### 3. ä½¿ç”¨ä½ çš„ Reranker

```bash
# æ›¿æ¢æ¨¡å‹åè¿è¡Œ
python bright_evaluator.py --method custom --reranker_path /path/to/your/model --all
```

---

## ğŸ”„ å®Œæ•´æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BRIGHT è¯„ä¼°ä¸æäº¤æµç¨‹                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 1. æ•°æ®åŠ è½½   â”‚ â”€â–º â”‚ 2. ç¬¬ä¸€é˜¶æ®µ   â”‚ â”€â–º â”‚ 3. ç¬¬äºŒé˜¶æ®µ   â”‚ â”€â–º â”‚ 4. è¯„ä¼°    â”‚ â”‚
â”‚  â”‚              â”‚    â”‚    æ£€ç´¢       â”‚    â”‚    é‡æ’       â”‚    â”‚            â”‚ â”‚
â”‚  â”‚ load_dataset â”‚    â”‚    BM25      â”‚    â”‚  ä½ çš„æ¨¡å‹     â”‚    â”‚ pytrec_evalâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                  â”‚                    â”‚                  â”‚        â”‚
â”‚         â–¼                  â–¼                    â–¼                  â–¼        â”‚
â”‚    12ä¸ªæ•°æ®é›†          Top-1000å€™é€‰        é‡æ’Top-100        nDCG@10 ç­‰    â”‚
â”‚                                                                             â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ 5. ç”ŸæˆæŠ¥å‘Š   â”‚ â”€â–º â”‚ 6. å‘é€é‚®ä»¶   â”‚ â”€â–º â”‚ 7. ä¸Šæ¦œ!      â”‚                  â”‚
â”‚  â”‚              â”‚    â”‚              â”‚    â”‚              â”‚                  â”‚
â”‚  â”‚ SUBMISSION_  â”‚    â”‚ suhongjin96  â”‚    â”‚ BRIGHT       â”‚                  â”‚
â”‚  â”‚ REPORT.md    â”‚    â”‚ @gmail.com   â”‚    â”‚ Leaderboard  â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š æ•°æ®æ ¼å¼è¯´æ˜

### BRIGHT æ•°æ®é›†ç»“æ„

```python
# åŠ è½½æ–¹å¼
from datasets import load_dataset

# Examples: åŒ…å«æŸ¥è¯¢å’Œæ ‡æ³¨
examples = load_dataset('xlangai/bright', 'examples')['biology']
# å­—æ®µ:
# - id: æŸ¥è¯¢ID (str)
# - query: æŸ¥è¯¢æ–‡æœ¬ (str)
# - gold_ids: ç›¸å…³æ–‡æ¡£IDåˆ—è¡¨ (List[str])
# - gold_ids_long: é•¿æ–‡æ¡£è®¾ç½®çš„ç›¸å…³ID (List[str])
# - excluded_ids: éœ€æ’é™¤çš„æ–‡æ¡£ID (List[str])

# Documents: æ–‡æ¡£åº“
documents = load_dataset('xlangai/bright', 'documents')['biology']
# å­—æ®µ:
# - id: æ–‡æ¡£ID (str)
# - content: æ–‡æ¡£å†…å®¹ (str)
```

### 12ä¸ªè¯„ä¼°ä»»åŠ¡

| ä»»åŠ¡å | é¢†åŸŸ | æŸ¥è¯¢æ•° | æ–‡æ¡£æ•° | éš¾åº¦ |
|--------|------|--------|--------|------|
| biology | ç”Ÿç‰©å­¦ | ~100 | ~57K | ä¸­ |
| earth_science | åœ°çƒç§‘å­¦ | ~80 | ~35K | ä¸­ |
| economics | ç»æµå­¦ | ~100 | ~40K | ä¸­ |
| psychology | å¿ƒç†å­¦ | ~90 | ~45K | ä¸­ |
| robotics | æœºå™¨äºº | ~70 | ~30K | ä¸­ |
| stackoverflow | ç¼–ç¨‹é—®ç­” | ~150 | ~60K | é«˜ |
| sustainable_living | å¯æŒç»­ç”Ÿæ´» | ~60 | ~25K | ä½ |
| leetcode | ç®—æ³•é¢˜ | ~140 | ~50K | é«˜ |
| pony | ç¼–ç¨‹è¯­è¨€ | ~50 | ~20K | ä¸­ |
| aops | æ•°å­¦ç«èµ› | ~100 | ~40K | é«˜ |
| theoremqa_questions | å®šç†é—®ç­” | ~80 | ~30K | é«˜ |
| theoremqa_theorems | å®šç† | ~70 | ~25K | é«˜ |

---

## ğŸ”§ å¦‚ä½•æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹

### æ–¹æ³•1ï¼šä¿®æ”¹ `PrincipleRankReranker` ç±»

ç¼–è¾‘ `bright_evaluator.py` ä¸­çš„ `PrincipleRankReranker` ç±»ï¼š

```python
class PrincipleRankReranker(BaseReranker):
    """ä½ çš„ PrincipleRank Reranker å®ç°"""
    
    def __init__(self, model_path: str = None, use_rubric: bool = True):
        self.model_path = model_path
        self.use_rubric = use_rubric
        
        # ============================================
        # TODO: åœ¨è¿™é‡ŒåŠ è½½ä½ çš„æ¨¡å‹
        # ============================================
        from vllm import LLM, SamplingParams
        self.llm = LLM(
            model=model_path,
            tensor_parallel_size=1,
            max_model_len=32768,
        )
        self.sampling_params = SamplingParams(
            temperature=0.0,
            max_tokens=2048,
        )
        
        print(f"[PrincipleRank] æ¨¡å‹åŠ è½½å®Œæˆ: {model_path}")
    
    def _build_prompt(self, query: str, documents: List[str], rubric: str) -> str:
        """
        æ„å»ºè¾“å…¥ Prompt
        
        æ ¹æ®ä½ çš„æ¨¡å‹è®­ç»ƒæ ¼å¼è°ƒæ•´
        """
        # ============================================
        # TODO: æ ¹æ®ä½ çš„è®­ç»ƒæ ¼å¼è°ƒæ•´ prompt
        # ============================================
        doc_str = "\n".join([f"[{i+1}] {doc[:500]}" for i, doc in enumerate(documents)])
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£æ’åºä¸“å®¶ã€‚è¯·æ ¹æ®æŸ¥è¯¢å’Œè¯„ä¼°å‡†åˆ™ï¼Œå¯¹å€™é€‰æ–‡æ¡£è¿›è¡Œæ’åºã€‚

æŸ¥è¯¢: {query}

è¯„ä¼°å‡†åˆ™:
{rubric}

å€™é€‰æ–‡æ¡£:
{doc_str}

è¯·æŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½æ’åºï¼Œè¾“å‡ºæ ¼å¼ä¸º: [3] > [1] > [5] > [2] > [4]

<think>
åˆ†ææŸ¥è¯¢æ„å›¾å’Œå„æ–‡æ¡£çš„ç›¸å…³æ€§...
</think>
<answer>
"""
        return prompt
    
    def rerank(
        self,
        query: str,
        documents: List[str],
        doc_ids: List[str],
        **kwargs
    ) -> List[Tuple[str, float]]:
        """
        æ‰§è¡Œé‡æ’åº
        """
        # ç”Ÿæˆå‡†åˆ™
        rubric = self._generate_rubric(query) if self.use_rubric else ""
        
        # æ„å»º prompt
        prompt = self._build_prompt(query, documents, rubric)
        
        # ============================================
        # TODO: è°ƒç”¨ä½ çš„æ¨¡å‹
        # ============================================
        outputs = self.llm.generate([prompt], self.sampling_params)
        output_text = outputs[0].outputs[0].text
        
        # è§£ææ’åºç»“æœ
        return self._parse_ranking(output_text, doc_ids)
```

### æ–¹æ³•2ï¼šç›´æ¥ä½¿ç”¨å·²æœ‰çš„ ReasonRank ä»£ç 

å¦‚æœä½ æƒ³å¤ç”¨é¡¹ç›®ä¸­å·²æœ‰çš„ `run_rank_llm.py` é€»è¾‘ï¼š

```python
# bright_evaluator.py ä¸­æ·»åŠ 

import sys
sys.path.append('/Users/changhao/xhs_paper/PrincipleReasonRank')

from run_rank_llm import Arguments
from rerank.rank_listwise_os_llm import RankListwiseOSLLM
from rerank.reranker import Reranker

class ExistingReranker(BaseReranker):
    """ä½¿ç”¨é¡¹ç›®å·²æœ‰çš„ Reranker"""
    
    def __init__(self, model_path: str):
        args = Arguments(
            model_path=model_path,
            context_size=32768,
            window_size=20,
            # ... å…¶ä»–å‚æ•°
        )
        agent = RankListwiseOSLLM(args=args, ...)
        self.reranker = Reranker(agent)
    
    def rerank(self, query, documents, doc_ids, **kwargs):
        # è°ƒç”¨å·²æœ‰çš„ reranker é€»è¾‘
        ...
```

### æ–¹æ³•3ï¼šä½¿ç”¨ç¼“å­˜çš„æ£€ç´¢ç»“æœ

å¦‚æœä½ å·²ç»æœ‰æ£€ç´¢ç»“æœï¼ˆåœ¨ `runs/` ç›®å½•ï¼‰ï¼Œå¯ä»¥ç›´æ¥åŠ è½½ï¼š

```python
# åŠ è½½å·²æœ‰çš„æ£€ç´¢ç»“æœ
def load_cached_retrieval_results(task: str, method: str = "bm25_gpt4cot") -> Dict:
    """
    åŠ è½½ç¼“å­˜çš„æ£€ç´¢ç»“æœ
    
    æ–‡ä»¶è·¯å¾„: runs/{task}/{method}_top100.txt
    """
    result_path = f"../runs/{task}/{method}_top100.txt"
    
    results = {}
    with open(result_path, 'r') as f:
        for line in f:
            qid, _, docid, rank, score, _ = line.strip().split()
            if qid not in results:
                results[qid] = {}
            results[qid][docid] = float(score)
    
    return results
```

---

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡è¯¦è§£

### ä¸»è¦æŒ‡æ ‡

| æŒ‡æ ‡ | å…¨ç§° | è¯´æ˜ | BRIGHT ä½¿ç”¨ |
|------|------|------|-------------|
| **nDCG@10** | Normalized Discounted Cumulative Gain | ä¸»è¦æ’åºæŒ‡æ ‡ï¼Œè€ƒè™‘ä½ç½®åŠ æƒ | âœ… ä¸»æŒ‡æ ‡ |
| Recall@1 | å¬å›ç‡@1 | Top-1 å‘½ä¸­ç‡ | âœ… é•¿æ–‡æ¡£æŒ‡æ ‡ |
| MRR | Mean Reciprocal Rank | ç¬¬ä¸€ä¸ªæ­£ç¡®ç»“æœçš„å¹³å‡æ’åå€’æ•° | å‚è€ƒ |

### åˆ†æ•°è®¡ç®—ç¤ºä¾‹

```python
# å‡è®¾æŸ¥è¯¢æœ‰3ä¸ªç›¸å…³æ–‡æ¡£: doc_A, doc_B, doc_C
# ä½ çš„æ¨¡å‹è¿”å›æ’åº: [doc_X, doc_A, doc_Y, doc_B, doc_Z, ...]

# nDCG@10 è®¡ç®—:
# DCG = rel_1/log2(2) + rel_2/log2(3) + rel_3/log2(4) + ...
#     = 0/1 + 1/1.58 + 0/2 + 1/2.32 + ...
# IDCG (ç†æƒ³æƒ…å†µ) = 1/1 + 1/1.58 + 1/2 + ...
# nDCG = DCG / IDCG

# ä»£ç ä¸­çš„è®¡ç®—
from pytrec_eval import RelevanceEvaluator

qrels = {"q1": {"doc_A": 1, "doc_B": 1, "doc_C": 1}}
scores = {"q1": {"doc_X": 0.9, "doc_A": 0.8, "doc_Y": 0.7, "doc_B": 0.6, "doc_Z": 0.5}}

evaluator = RelevanceEvaluator(qrels, {"ndcg_cut.10"})
results = evaluator.evaluate(scores)
print(results["q1"]["ndcg_cut_10"])  # ä¾‹å¦‚: 0.7523
```

---

## ğŸ“ ç”Ÿæˆæäº¤æŠ¥å‘Š

### è‡ªåŠ¨ç”Ÿæˆ

```bash
python bright_evaluator.py --method bm25 --all --generate_report
```

ä¼šåœ¨ `outputs/` ç›®å½•ç”Ÿæˆ `SUBMISSION_REPORT.md`ã€‚

### æŠ¥å‘Šæ¨¡æ¿

```markdown
# BRIGHT Benchmark æäº¤æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯

- **æ¨¡å‹åç§°**: PrincipleRank-7B
- **åŸºåº§æ¨¡å‹**: Qwen2.5-7B-Instruct
- **è®­ç»ƒæ–¹æ³•**: SFT + DPO
- **æ˜¯å¦ä½¿ç”¨æ¨ç†**: æ˜¯ (CoT)

## åˆ†æ•°æ±‡æ€»

| ä»»åŠ¡ | nDCG@10 |
|------|---------|
| biology | 12.34 |
| economics | 15.67 |
| ... | ... |
| **å¹³å‡** | **XX.XX** |

## æ¨¡å‹æè¿°

PrincipleRank æ˜¯ä¸€ä¸ªå‡†åˆ™å¼•å¯¼çš„ç”Ÿæˆå¼æ’åºæ¨¡å‹...

## å¼€æºé“¾æ¥

GitHub: https://github.com/xxx/PrincipleRank
```

---

## ğŸ“§ æäº¤æ–¹æ³•

### Step 1: å®Œæˆè¯„ä¼°

```bash
# ç¡®ä¿è¯„ä¼°äº†æ‰€æœ‰12ä¸ªä»»åŠ¡
python bright_evaluator.py --method custom --all --generate_report
```

### Step 2: æ£€æŸ¥è¾“å‡ºæ–‡ä»¶

```bash
outputs/
â”œâ”€â”€ biology_long_False/
â”‚   â”œâ”€â”€ scores.json      # æ¯ä¸ªæŸ¥è¯¢çš„åˆ†æ•°
â”‚   â””â”€â”€ metrics.json     # è¯„ä¼°æŒ‡æ ‡
â”œâ”€â”€ economics_long_False/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ...
â””â”€â”€ SUBMISSION_REPORT.md  # æäº¤æŠ¥å‘Š
```

### Step 3: å‘é€é‚®ä»¶

**æ”¶ä»¶äºº**: suhongjin96@gmail.com

**é‚®ä»¶ä¸»é¢˜**: BRIGHT Benchmark Submission - [ä½ çš„æ¨¡å‹åç§°]

**é‚®ä»¶å†…å®¹**:

```
Hi,

I would like to submit my model to the BRIGHT benchmark.

Model Name: PrincipleRank-7B
Model Size: 7B parameters
Uses LLM Reasoning: Yes

## Results

| Task | nDCG@10 |
|------|---------|
| biology | XX.XX |
| economics | XX.XX |
| ... | ... |
| Average | XX.XX |

## Model Description

[ç®€è¦æè¿°ä½ çš„æ¨¡å‹]

## Code Repository (Optional)

https://github.com/xxx/xxx

Best regards,
[ä½ çš„åå­—]
```

**é™„ä»¶**: 
- `SUBMISSION_REPORT.md`
- ï¼ˆå¯é€‰ï¼‰å®Œæ•´çš„ metrics.json æ–‡ä»¶

### Step 4: ç­‰å¾…ä¸Šæ¦œ

æäº¤åé€šå¸¸ 1-3 ä¸ªå·¥ä½œæ—¥ä¼šè¢«æ·»åŠ åˆ° [Leaderboard](https://brightbenchmark.github.io/)ã€‚

---

## â“ å¸¸è§é—®é¢˜

### Q1: æ•°æ®ä¸‹è½½å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

```bash
# ä½¿ç”¨é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
python bright_evaluator.py --all
```

### Q2: GPU å†…å­˜ä¸è¶³ï¼Ÿ

```python
# å‡å° batch size
self.llm = LLM(
    model=model_path,
    tensor_parallel_size=1,
    gpu_memory_utilization=0.85,  # é™ä½å†…å­˜ä½¿ç”¨
    max_model_len=16384,  # å‡å°ä¸Šä¸‹æ–‡é•¿åº¦
)
```

### Q3: å¦‚ä½•ä½¿ç”¨é•¿æ–‡æ¡£è®¾ç½®ï¼Ÿ

```bash
# æ·»åŠ  --long_context å‚æ•°
python bright_evaluator.py --method custom --all --long_context
```

é•¿æ–‡æ¡£è®¾ç½®ä½¿ç”¨ Recall@1 ä½œä¸ºä¸»è¦æŒ‡æ ‡ï¼ˆè€Œé nDCG@10ï¼‰ã€‚

### Q4: åˆ†æ•°å¾ˆä½æ€ä¹ˆåŠï¼Ÿ

1. **æ£€æŸ¥è¾“å‡ºæ ¼å¼**: ç¡®ä¿æ¨¡å‹è¾“å‡ºå¯ä»¥è¢«æ­£ç¡®è§£æä¸º `[3] > [1] > [5] > ...` æ ¼å¼
2. **æ£€æŸ¥æ¨ç†é“¾**: å¦‚æœä½¿ç”¨ CoTï¼Œç¡®ä¿ `<think>...</think>` å’Œ `<answer>...</answer>` æ ¼å¼æ­£ç¡®
3. **è°ƒè¯•å•ä¸ªæŸ¥è¯¢**: æ‰“å°æ¨¡å‹è¾“å…¥è¾“å‡ºï¼Œæ£€æŸ¥æ˜¯å¦åˆç†

```python
# è°ƒè¯•ä»£ç 
def debug_single_query(query, docs, model):
    prompt = build_prompt(query, docs)
    print("=== INPUT ===")
    print(prompt[:1000])
    
    output = model.generate(prompt)
    print("=== OUTPUT ===")
    print(output)
    
    ranking = parse_ranking(output)
    print("=== PARSED RANKING ===")
    print(ranking)
```

### Q5: èƒ½å¦åªæäº¤éƒ¨åˆ†ä»»åŠ¡çš„åˆ†æ•°ï¼Ÿ

ä¸è¡Œã€‚BRIGHT è¦æ±‚æäº¤æ‰€æœ‰ 12 ä¸ªçŸ­æ–‡æ¡£ä»»åŠ¡çš„åˆ†æ•°æ‰èƒ½ä¸Šæ¦œã€‚

---

## ğŸ“ æ–‡ä»¶æ¸…å•

```
bright_eval/
â”œâ”€â”€ README.md                 # æœ¬æ–‡æ¡£
â”œâ”€â”€ bright_evaluator.py       # ä¸»è¯„ä¼°è„šæœ¬
â”œâ”€â”€ custom_retriever.py       # æ—§ç‰ˆæ£€ç´¢å™¨ï¼ˆå‚è€ƒï¼‰
â”œâ”€â”€ run_custom.py             # æ—§ç‰ˆè¯„ä¼°è„šæœ¬ï¼ˆå‚è€ƒï¼‰
â””â”€â”€ outputs/                  # è¯„ä¼°ç»“æœè¾“å‡º
    â”œâ”€â”€ {task}_long_False/
    â”‚   â”œâ”€â”€ scores.json
    â”‚   â””â”€â”€ metrics.json
    â””â”€â”€ SUBMISSION_REPORT.md
```

---

## ğŸ”— ç›¸å…³é“¾æ¥

- [BRIGHT å®˜ç½‘](https://brightbenchmark.github.io/)
- [BRIGHT è®ºæ–‡](https://arxiv.org/abs/2407.12883)
- [BRIGHT GitHub](https://github.com/xlang-ai/BRIGHT)
- [HuggingFace æ•°æ®é›†](https://huggingface.co/datasets/xlangai/bright)

---

*æœ€åæ›´æ–°: 2026-01-25*
