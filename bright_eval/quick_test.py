#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•è„šæœ¬ - éªŒè¯ç¯å¢ƒå®‰è£…æ˜¯å¦æ­£ç¡®

è¿è¡Œæ–¹å¼:
    python quick_test.py
"""

import sys

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–æ˜¯å¦å®‰è£…"""
    
    print("=" * 60)
    print("BRIGHT Benchmark ç¯å¢ƒæ£€æŸ¥")
    print("=" * 60)
    
    dependencies = {
        "datasets": "datasets (HuggingFace)",
        "pytrec_eval": "pytrec_eval",
        "rank_bm25": "rank_bm25",
        "numpy": "numpy",
        "tqdm": "tqdm",
    }
    
    optional_deps = {
        "transformers": "transformers",
        "torch": "torch",
        "vllm": "vllm",
    }
    
    all_ok = True
    
    print("\n[å¿…è¦ä¾èµ–]")
    for module, name in dependencies.items():
        try:
            __import__(module)
            print(f"  âœ… {name}")
        except ImportError:
            print(f"  âŒ {name} - è¯·è¿è¡Œ: pip install {module}")
            all_ok = False
    
    print("\n[å¯é€‰ä¾èµ– - ç”¨äº LLM æ¨ç†]")
    for module, name in optional_deps.items():
        try:
            __import__(module)
            print(f"  âœ… {name}")
        except ImportError:
            print(f"  âš ï¸ {name} - æœªå®‰è£… (å¦‚éœ€ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹ï¼Œè¯·å®‰è£…)")
    
    return all_ok


def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ•°æ®åŠ è½½")
    print("=" * 60)
    
    try:
        from datasets import load_dataset
        
        print("\n[åŠ è½½ BRIGHT æ•°æ®é›† - biology (å¯èƒ½éœ€è¦ä¸‹è½½)...]")
        
        # å°è¯•åŠ è½½ä¸€å°éƒ¨åˆ†æ•°æ®
        examples = load_dataset('xlangai/bright', 'examples', split='biology')
        print(f"  âœ… Examples åŠ è½½æˆåŠŸ: {len(examples)} æ¡")
        
        documents = load_dataset('xlangai/bright', 'documents', split='biology')
        print(f"  âœ… Documents åŠ è½½æˆåŠŸ: {len(documents)} æ¡")
        
        # æ˜¾ç¤ºç¤ºä¾‹
        sample = examples[0]
        print(f"\n  ç¤ºä¾‹æŸ¥è¯¢: {sample['query'][:100]}...")
        print(f"  ç›¸å…³æ–‡æ¡£æ•°: {len(sample['gold_ids'])}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        print("\n  æç¤º: å¦‚æœæ˜¯ç½‘ç»œé—®é¢˜ï¼Œå¯ä»¥å°è¯•:")
        print("    export HF_ENDPOINT=https://hf-mirror.com")
        return False


def test_bm25():
    """æµ‹è¯• BM25 æ£€ç´¢å™¨"""
    
    print("\n" + "=" * 60)
    print("æµ‹è¯• BM25 æ£€ç´¢å™¨")
    print("=" * 60)
    
    try:
        from rank_bm25 import BM25Okapi
        
        # ç®€å•æµ‹è¯•
        corpus = [
            "Hello there good man!",
            "It is quite windy in London",
            "How is the weather today?",
            "The weather in London is rainy",
        ]
        
        tokenized_corpus = [doc.split(" ") for doc in corpus]
        bm25 = BM25Okapi(tokenized_corpus)
        
        query = "weather in London"
        scores = bm25.get_scores(query.split(" "))
        
        print(f"  âœ… BM25 æµ‹è¯•æˆåŠŸ")
        print(f"  æŸ¥è¯¢: '{query}'")
        print(f"  åˆ†æ•°: {scores}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ BM25 æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_evaluation():
    """æµ‹è¯•è¯„ä¼°æŒ‡æ ‡è®¡ç®—"""
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•è¯„ä¼°æŒ‡æ ‡è®¡ç®—")
    print("=" * 60)
    
    try:
        import pytrec_eval
        
        # æ¨¡æ‹Ÿæ•°æ®
        qrels = {
            "q1": {"doc1": 1, "doc2": 1, "doc3": 0},
            "q2": {"doc4": 1, "doc5": 0},
        }
        
        results = {
            "q1": {"doc1": 0.9, "doc2": 0.7, "doc3": 0.3, "doc4": 0.1},
            "q2": {"doc4": 0.8, "doc5": 0.6, "doc1": 0.2},
        }
        
        evaluator = pytrec_eval.RelevanceEvaluator(qrels, {"ndcg_cut.10", "recall.10"})
        metrics = evaluator.evaluate(results)
        
        avg_ndcg = sum(m["ndcg_cut_10"] for m in metrics.values()) / len(metrics)
        
        print(f"  âœ… è¯„ä¼°æµ‹è¯•æˆåŠŸ")
        print(f"  å¹³å‡ nDCG@10: {avg_ndcg:.4f}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ è¯„ä¼°æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    
    print("\n" + "ğŸ” " * 20)
    print("       BRIGHT Benchmark ç¯å¢ƒæµ‹è¯•")
    print("ğŸ” " * 20)
    
    results = []
    
    # 1. æ£€æŸ¥ä¾èµ–
    results.append(("ä¾èµ–æ£€æŸ¥", check_dependencies()))
    
    # 2. æµ‹è¯• BM25
    results.append(("BM25 æ£€ç´¢", test_bm25()))
    
    # 3. æµ‹è¯•è¯„ä¼°
    results.append(("æŒ‡æ ‡è¯„ä¼°", test_evaluation()))
    
    # 4. æµ‹è¯•æ•°æ®åŠ è½½ (å¯é€‰ï¼Œå› ä¸ºéœ€è¦ä¸‹è½½)
    print("\næ˜¯å¦æµ‹è¯•æ•°æ®åŠ è½½? (éœ€è¦ä¸‹è½½çº¦ 500MB æ•°æ®) [y/N]: ", end="")
    try:
        answer = input().strip().lower()
        if answer == 'y':
            results.append(("æ•°æ®åŠ è½½", test_data_loading()))
    except:
        pass
    
    # æ±‡æ€»
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ±‡æ€»")
    print("=" * 60)
    
    all_passed = True
    for name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"  {name}: {status}")
        if not passed:
            all_passed = False
    
    print("\n" + "=" * 60)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! ç¯å¢ƒé…ç½®æ­£ç¡®ã€‚")
        print("\nä¸‹ä¸€æ­¥:")
        print("  1. è¿è¡ŒåŸºçº¿è¯„ä¼°: python bright_evaluator.py --method bm25 --quick")
        print("  2. æ›¿æ¢ä½ çš„æ¨¡å‹: ç¼–è¾‘ bright_evaluator.py ä¸­çš„ PrincipleRankReranker")
        print("  3. ç”Ÿæˆæäº¤æŠ¥å‘Š: python bright_evaluator.py --method custom --all --generate_report")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·æ ¹æ®æç¤ºå®‰è£…ç¼ºå¤±çš„ä¾èµ–ã€‚")
        print("\nå¿«é€Ÿå®‰è£…æ‰€æœ‰ä¾èµ–:")
        print("  pip install -r requirements.txt")
    
    print("=" * 60)
    
    return 0 if all_passed else 1


if __name__ == "__main__":
    sys.exit(main())
