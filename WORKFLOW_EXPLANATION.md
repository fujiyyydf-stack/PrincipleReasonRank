# PrincipleRank å·¥ä½œæµç¨‹è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†è§£é‡Š BRIGHT benchmark çš„æ‰“åˆ†é€»è¾‘ã€æ•°æ®æµç¨‹ã€ä»¥åŠè®­ç»ƒ/æ¨ç†æ—¶å€™é€‰é›†çš„å¤„ç†æ–¹å¼ã€‚

---

## ä¸€ã€BRIGHT Benchmark æ‰“åˆ†é€»è¾‘è¯¦è§£

### 1.1 æ•´ä½“æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           BRIGHT è¯„ä¼°æµç¨‹                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Query        First-Stage Retriever        Reranker           Evaluation    â”‚
â”‚                                                                             â”‚
â”‚  "How to..."  â”€â”€â–º BM25/Dense Retriever â”€â”€â–º ä½ çš„æ¨¡å‹ â”€â”€â–º  pytrec_eval        â”‚
â”‚                   è¿”å› Top-100             é‡æ’ Top-K      è®¡ç®— nDCG@10      â”‚
â”‚                                                                             â”‚
â”‚                   {qid: {did: score}}     {qid: {did: score}}               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 åˆ†æ•°æ ¼å¼è¯´æ˜

**è¾“å…¥ç»™è¯„ä¼°å™¨çš„æ ¼å¼**ï¼š
```python
# scores å­—å…¸ï¼šæ¯ä¸ª query å¯¹åº”å…¶å€™é€‰æ–‡æ¡£çš„åˆ†æ•°
scores = {
    "query_1": {
        "doc_1": 0.95,   # åˆ†æ•°è¶Šé«˜ï¼Œæ’åè¶Šé å‰
        "doc_2": 0.82,
        "doc_3": 0.45,
        ...
    },
    "query_2": {...},
}

# qrels å­—å…¸ï¼šground truthï¼Œ1 è¡¨ç¤ºç›¸å…³ï¼Œ0 è¡¨ç¤ºä¸ç›¸å…³
qrels = {
    "query_1": {
        "doc_1": 1,   # è¿™ä¸ªæ–‡æ¡£æ˜¯ç›¸å…³çš„
        "doc_5": 1,
        ...
    }
}

# è¯„ä¼°
from pytrec_eval import RelevanceEvaluator
evaluator = RelevanceEvaluator(qrels, {"ndcg_cut.10"})
results = evaluator.evaluate(scores)
```

### 1.3 ç”Ÿæˆå¼ Reranker çš„åˆ†æ•°è½¬æ¢é—®é¢˜

**æ ¸å¿ƒé—®é¢˜**ï¼šç”Ÿæˆå¼æ¨¡å‹è¾“å‡ºçš„æ˜¯**æ’åºé¡ºåº**ï¼ˆå¦‚ `[3] > [1] > [5] > [2] > [4]`ï¼‰ï¼Œè€Œä¸æ˜¯æ•°å€¼åˆ†æ•°ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šå°†æ’åºä½ç½®è½¬æ¢ä¸ºåˆ†æ•°

```python
def convert_ranking_to_scores(ranking_order: List[str], doc_ids: List[str]) -> Dict[str, float]:
    """
    å°†æ’åºé¡ºåºè½¬æ¢ä¸ºåˆ†æ•°
    
    Args:
        ranking_order: æ¨¡å‹è¾“å‡ºçš„æ’åºï¼Œå¦‚ ["doc_3", "doc_1", "doc_5", ...]
        doc_ids: æ‰€æœ‰å€™é€‰æ–‡æ¡£ID
    
    Returns:
        scores: {doc_id: score}
    """
    scores = {}
    n = len(ranking_order)
    
    for rank, doc_id in enumerate(ranking_order):
        # æ–¹æ³•1: å€’æ•°æ’ååˆ†æ•° (å¸¸ç”¨)
        scores[doc_id] = 1.0 / (rank + 1)
        
        # æ–¹æ³•2: çº¿æ€§åˆ†æ•°
        # scores[doc_id] = (n - rank) / n
        
        # æ–¹æ³•3: æŒ‡æ•°è¡°å‡
        # scores[doc_id] = math.exp(-0.1 * rank)
    
    # æœªè¢«æ’åºçš„æ–‡æ¡£ç»™ä¸€ä¸ªå¾ˆä½çš„åˆ†æ•°
    for doc_id in doc_ids:
        if doc_id not in scores:
            scores[doc_id] = 0.0
    
    return scores
```

**åœ¨ä½ çš„ä»£ç ä¸­çš„ä½ç½®**ï¼š
`rerank/rank_listwise_os_llm.py` ä¸­çš„ `sliding_windows` æ–¹æ³•ä¼šå¤„ç†è¿™ä¸ªè½¬æ¢ï¼Œå€™é€‰æ–‡æ¡£çš„ `.score` å±æ€§ä¼šè¢«æ›´æ–°ã€‚

---

## äºŒã€å…³äº Meta-ranking æ ¡å‡†æœºåˆ¶çš„å»ºè®®

### 2.1 å½“å‰è®¾è®¡çš„å¤æ‚æ€§

ä½ è®ºæ–‡ä¸­çš„ Meta-ranking è®¾è®¡ï¼š
```
ç”Ÿæˆå¼æ¨¡å‹ â†’ æ’åº + æ¨ç† â†’ Meta æ¨¡å‹æ‰“åˆ† â†’ åŠ æƒèåˆ
                 â†“
          Qwen3-Reranker
```

**æ¨ç†æ—¶çš„é—®é¢˜**ï¼š
1. éœ€è¦**ä¸¤æ¬¡æ¨¡å‹è°ƒç”¨**ï¼ˆç”Ÿæˆå¼ + åˆ¤åˆ«å¼ï¼‰
2. éœ€è¦ä»ç”Ÿæˆå¼è¾“å‡º**è§£ææ’åºç»“æœ**
3. éœ€è¦è®¾è®¡**èåˆæƒé‡** Î±
4. å¢åŠ **æ¨ç†å»¶è¿Ÿ**

### 2.2 å»ºè®®ï¼šç®€åŒ–æ–¹æ¡ˆ

**æ–¹æ¡ˆ Aï¼šåˆ é™¤ Meta-rankingï¼ˆæ¨èåˆæœŸé‡‡ç”¨ï¼‰**

```markdown
ä¿®æ”¹è®ºæ–‡ç¬¬ 4.4 èŠ‚ï¼Œå°† Meta-ranking æ”¹ä¸º"å¯é€‰æ‰©å±•"æˆ–"æœªæ¥å·¥ä½œ"ï¼š

åŸæ–‡ï¼š
**ï¼ˆ3ï¼‰æå‡ºäº†ç”Ÿæˆ-åˆ¤åˆ«æ··åˆçš„ Meta-ranking æ ¡å‡†æœºåˆ¶**

ä¿®æ”¹ä¸ºï¼š
**ï¼ˆ3ï¼‰è®¾è®¡äº†è‡ªé€‚åº”æ¨ç†æ·±åº¦æœºåˆ¶**ï¼ˆåŸ 4.5 èŠ‚å†…å®¹æå‡ï¼‰

æˆ–ç›´æ¥åˆ é™¤è¿™ä¸€è´¡çŒ®ç‚¹ï¼Œèšç„¦äºå‰ä¸¤ç‚¹ã€‚
```

**æ–¹æ¡ˆ Bï¼šç®€åŒ– Meta-rankingï¼ˆå¦‚æœæƒ³ä¿ç•™ï¼‰**

```python
# ç®€åŒ–ç‰ˆï¼šä»…åœ¨æ ¼å¼è§£æå¤±è´¥æ—¶ä½¿ç”¨åˆ¤åˆ«å¼æ¨¡å‹
def rerank_with_fallback(query, docs, gen_model, disc_model):
    # 1. å°è¯•ç”Ÿæˆå¼æ’åº
    gen_output = gen_model.generate(query, docs)
    ranking = parse_ranking(gen_output)
    
    # 2. å¦‚æœè§£ææˆåŠŸï¼Œç›´æ¥è¿”å›
    if ranking is not None:
        return convert_to_scores(ranking)
    
    # 3. è§£æå¤±è´¥ï¼Œå›é€€åˆ°åˆ¤åˆ«å¼æ¨¡å‹
    scores = disc_model.score(query, docs)
    return scores
```

---

## ä¸‰ã€å€™é€‰é›†çš„è·å–ä¸å¤„ç†

### 3.1 æ ¸å¿ƒåŸåˆ™ï¼šæ£€ç´¢åªåšä¸€æ¬¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         å€™é€‰é›†å¤„ç†æµç¨‹                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   [ä¸€æ¬¡æ€§æ£€ç´¢]                    [ç¼“å­˜æ–‡ä»¶]           [è®­ç»ƒ/æ¨ç†å¤ç”¨]        â”‚
â”‚                                                                             â”‚
â”‚   Query Corpus  â”€â”€â–º Retriever â”€â”€â–º runs/xxx.txt  â”€â”€â–º SFTè®­ç»ƒ               â”‚
â”‚                      (BM25ç­‰)      (TRECæ ¼å¼)    â”€â”€â–º DPOè®­ç»ƒ               â”‚
â”‚                                                    â”€â”€â–º æ¨ç†è¯„ä¼°              â”‚
â”‚                                                                             â”‚
â”‚   åªæ‰§è¡Œä¸€æ¬¡!                                        å¤šæ¬¡å¤ç”¨                 â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 ä½ é¡¹ç›®ä¸­çš„æ£€ç´¢ç»“æœä½ç½®

```bash
PrincipleReasonRank/
â”œâ”€â”€ runs/                          # æ£€ç´¢ç»“æœç¼“å­˜
â”‚   â”œâ”€â”€ biology/
â”‚   â”‚   â””â”€â”€ bm25_gpt4cot_top100.txt   # BM25 æ£€ç´¢ç»“æœ
â”‚   â”œâ”€â”€ economics/
â”‚   â”‚   â””â”€â”€ bm25_gpt4cot_top100.txt
â”‚   â”œâ”€â”€ leetcode/
â”‚   â”‚   â””â”€â”€ bm25_top100.txt
â”‚   â””â”€â”€ ...                        # å…¶ä»–æ•°æ®é›†
```

**æ–‡ä»¶æ ¼å¼ï¼ˆTREC æ ¼å¼ï¼‰**ï¼š
```
query_id Q0 doc_id rank score run_name
q1 Q0 doc_123 1 25.4321 bm25
q1 Q0 doc_456 2 24.1234 bm25
q1 Q0 doc_789 3 23.5678 bm25
...
```

### 3.3 å¦‚ä½•ä½¿ç”¨æ›´å¥½çš„æ£€ç´¢å™¨

**å½“å‰ä»£ç æ”¯æŒçš„æ£€ç´¢æ–¹å¼**ï¼ˆè§ `run_rank_llm.py`ï¼‰ï¼š

```python
class RetrievalMethod(Enum):
    BM25 = "bm25"                              # ç¨€ç–æ£€ç´¢
    SPLADE_P_P_ENSEMBLE_DISTIL = "SPLADE++..."  # å­¦ä¹ å‹ç¨€ç–
    D_BERT_KD_TASB = "distilbert_tas_b"         # ç¨ å¯†æ£€ç´¢
    E5_MISTRAL = "e5-mistral-7b-instruct"       # å¤§æ¨¡å‹ Embedding
    REASONIR = "reasonir"                       # æ¨ç†å¢å¼ºæ£€ç´¢
    RaDeR = "RaDeR-..."                         # SOTA æ–¹æ³•
```

**æ¨èçš„æ£€ç´¢å™¨é€‰æ‹©**ï¼š

| åœºæ™¯ | æ¨èæ£€ç´¢å™¨ | åŸå›  |
|------|-----------|------|
| å¿«é€ŸåŸºçº¿ | BM25 | é€Ÿåº¦å¿«ï¼Œæ•ˆæœç¨³å®š |
| BRIGHT æ•°æ®é›† | BM25 + GPT4 CoT | å®˜æ–¹æ¨èï¼Œå·²æœ‰ç¼“å­˜ |
| è¿½æ±‚ SOTA | ReasonIR / RaDeR | æ¨ç†å¢å¼ºï¼Œæ•ˆæœæœ€å¥½ |

### 3.4 ä¸€æ¬¡æ£€ç´¢ï¼Œå¤šæ¬¡å¤ç”¨çš„ä»£ç æµç¨‹

```python
# run_rank_llm.py ä¸­çš„é€»è¾‘

# 1. æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ£€ç´¢ç»“æœ
first_stage_run_path = f'runs/{dataset}/bm25_gpt4cot_top100.txt'

if os.path.exists(first_stage_run_path):
    # 2a. æœ‰ç¼“å­˜ï¼Œç›´æ¥åŠ è½½
    print(f'Loading first stage run from {first_stage_run_path}.')
    results = load_from_trec_file(first_stage_run_path)
else:
    # 2b. æ— ç¼“å­˜ï¼Œæ‰§è¡Œæ£€ç´¢å¹¶ä¿å­˜
    results = searcher.batch_search(queries, ...)
    save_to_trec_file(results, first_stage_run_path)

# 3. ä½¿ç”¨æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’
reranked = reranker.rerank_batch(results)
```

---

## å››ã€è®­ç»ƒæ•°æ®æ„é€ æµç¨‹

### 4.1 SFT æ•°æ®æ„é€ 

```python
def construct_sft_data(dataset_name: str, retriever: str = "bm25"):
    """
    æ„é€  SFT è®­ç»ƒæ•°æ®
    
    è¾“å…¥ï¼š
    - queries: æŸ¥è¯¢åˆ—è¡¨
    - æ£€ç´¢ç»“æœ: runs/{dataset}/bm25_top100.txt
    
    è¾“å‡ºï¼š
    - SFT è®­ç»ƒæ ·æœ¬: (query, docs, rubric) â†’ (reasoning, ranking)
    """
    
    # 1. åŠ è½½æ£€ç´¢ç»“æœï¼ˆåªåšä¸€æ¬¡æ£€ç´¢ï¼‰
    retrieval_results = load_retrieval_results(f"runs/{dataset_name}/{retriever}_top100.txt")
    
    # 2. å¯¹æ¯ä¸ª query æ„é€ è®­ç»ƒæ ·æœ¬
    sft_samples = []
    for qid, candidates in retrieval_results.items():
        query = queries[qid]
        
        # 3. é‡‡æ ·å€™é€‰å­é›†ï¼ˆé€šå¸¸ 20-30 ä¸ªï¼‰
        sampled_docs = sample_candidates(candidates, k=20)
        
        # 4. ç”Ÿæˆå‡†åˆ™ï¼ˆRubricï¼‰
        rubric = generate_rubric(query)  # ç”¨ä½ çš„ Agentic ç®¡çº¿
        
        # 5. ç”¨å¼ºæ¨¡å‹ç”Ÿæˆæ¨ç†å’Œæ’åº
        reasoning, ranking = strong_model.generate(query, sampled_docs, rubric)
        
        # 6. æ„é€ è®­ç»ƒæ ·æœ¬
        sft_samples.append({
            "input": format_input(query, sampled_docs, rubric),
            "output": f"<think>{reasoning}</think><answer>{ranking}</answer>"
        })
    
    return sft_samples
```

### 4.2 DPO æ•°æ®æ„é€ 

```python
def construct_dpo_data(sft_model, dataset_name: str):
    """
    æ„é€  DPO åå¥½å¯¹æ•°æ®
    """
    
    # 1. åŠ è½½ç›¸åŒçš„æ£€ç´¢ç»“æœ
    retrieval_results = load_retrieval_results(f"runs/{dataset_name}/bm25_top100.txt")
    
    dpo_pairs = []
    for qid, candidates in retrieval_results.items():
        query = queries[qid]
        sampled_docs = sample_candidates(candidates, k=20)
        rubric = get_rubric(qid)  # ä½¿ç”¨é¢„ç”Ÿæˆçš„ rubric
        
        # 2. ç”¨ SFT æ¨¡å‹é‡‡æ ·å¤šä¸ªè¾“å‡º
        outputs = []
        for _ in range(8):
            output = sft_model.generate(query, sampled_docs, rubric, temperature=0.8)
            outputs.append(output)
        
        # 3. ç”¨ Verifier è¯„åˆ†
        scores = verifier.score_batch(query, sampled_docs, rubric, outputs)
        
        # 4. é€‰æ‹©æœ€å¥½å’Œè¾ƒå·®çš„ä½œä¸ºåå¥½å¯¹
        best_idx = np.argmax(scores)
        worst_idx = np.argmin(scores)
        
        if scores[best_idx] - scores[worst_idx] > threshold:
            dpo_pairs.append({
                "input": format_input(query, sampled_docs, rubric),
                "chosen": outputs[best_idx],
                "rejected": outputs[worst_idx]
            })
    
    return dpo_pairs
```

### 4.3 æ¨ç†æ—¶çš„æµç¨‹

```python
def inference(model, dataset_name: str):
    """
    æ¨ç†è¯„ä¼°æµç¨‹
    """
    
    # 1. åŠ è½½ç›¸åŒçš„æ£€ç´¢ç»“æœï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    retrieval_results = load_retrieval_results(f"runs/{dataset_name}/bm25_top100.txt")
    
    all_scores = {}
    for qid, candidates in retrieval_results.items():
        query = queries[qid]
        
        # 2. å¯é€‰ï¼šåŠ è½½æˆ–ç”Ÿæˆ rubric
        rubric = get_rubric(qid)  # æˆ–è®¾ä¸º None å¦‚æœä¸ä½¿ç”¨å‡†åˆ™
        
        # 3. æ¨¡å‹é‡æ’
        output = model.generate(query, candidates, rubric)
        
        # 4. è§£ææ’åºç»“æœï¼Œè½¬æ¢ä¸ºåˆ†æ•°
        ranking = parse_ranking(output)
        scores = convert_to_scores(ranking, [c.docid for c in candidates])
        
        all_scores[qid] = scores
    
    # 5. è¯„ä¼°
    metrics = evaluate(all_scores, qrels)
    return metrics
```

---

## äº”ã€æ¨èçš„ç®€åŒ–å®ç°è·¯å¾„

### 5.1 Phase 1: åŸºç¡€ç‰ˆæœ¬ï¼ˆå»ºè®®å…ˆå®Œæˆï¼‰

```
ç›®æ ‡ï¼šå®ç°ä¸€ä¸ªèƒ½è·‘é€š BRIGHT è¯„ä¼°çš„åŸºç¡€ reranker

1. æ•°æ®å‡†å¤‡
   - ä½¿ç”¨ç°æœ‰çš„ BM25 æ£€ç´¢ç»“æœï¼ˆruns/ ç›®å½•ä¸‹å·²æœ‰ï¼‰
   - æš‚ä¸ç”Ÿæˆ Rubricï¼Œä½¿ç”¨é€šç”¨æŒ‡ä»¤

2. æ¨¡å‹è®­ç»ƒ
   - SFT: ç”¨ DeepSeek-R1 ç”Ÿæˆè®­ç»ƒæ•°æ®
   - è·³è¿‡ DPOï¼ˆåˆæœŸç®€åŒ–ï¼‰

3. è¯„ä¼°
   - åœ¨ BRIGHT 3 ä¸ªä»»åŠ¡ä¸Šæµ‹è¯•
   - å¯¹æ¯” BM25 baseline
```

### 5.2 Phase 2: å¢å¼ºç‰ˆæœ¬

```
ç›®æ ‡ï¼šåŠ å…¥å‡†åˆ™å¼•å¯¼å’Œåå¥½ä¼˜åŒ–

1. Agentic Rubric ç”Ÿæˆ
   - å®ç°å¤šæ™ºèƒ½ä½“ç®¡çº¿
   - ç”Ÿæˆ Query-Rubric æ•°æ®é›†

2. è®­ç»ƒå¢å¼º
   - Rubric-aware SFT
   - DPO with Verifier

3. å…¨é‡è¯„ä¼°
   - BRIGHT å…¨éƒ¨ 12 ä¸ªä»»åŠ¡
   - ä¸ ReasonRank å¯¹æ¯”
```

### 5.3 Phase 3: å®Œæ•´ç‰ˆæœ¬ï¼ˆå¯é€‰ï¼‰

```
ç›®æ ‡ï¼šå®ç°å®Œæ•´è®ºæ–‡æ–¹æ¡ˆ

1. Meta-rankingï¼ˆå¦‚æœéœ€è¦ï¼‰
2. è‡ªé€‚åº”æ¨ç†æ·±åº¦
3. æ¶ˆèå®éªŒ
```

---

## å…­ã€å…³é”®ä»£ç ä¿®æ”¹å»ºè®®

### 6.1 åˆ é™¤ Meta-ranking åçš„è®ºæ–‡ä¿®æ”¹

**ç¬¬ 4.4 èŠ‚æ•´ä½“åˆ é™¤æˆ–æ”¹ä¸º"æœªæ¥å·¥ä½œ"**

**ç¬¬å…­ç«  6.1 èŠ‚ä¿®æ”¹**ï¼š
```markdown
åŸæ–‡ï¼š
**ï¼ˆ3ï¼‰æå‡ºäº†ç”Ÿæˆ-åˆ¤åˆ«æ··åˆçš„ Meta-ranking æ ¡å‡†æœºåˆ¶**

ä¿®æ”¹ä¸ºï¼š
**ï¼ˆ3ï¼‰è®¾è®¡äº†é«˜æ•ˆçš„æ¨ç†æ·±åº¦è‡ªé€‚åº”ç­–ç•¥**ï¼ˆä¿æŒç®€æ´ï¼Œèšç„¦æ ¸å¿ƒè´¡çŒ®ï¼‰

æˆ–ï¼š
åˆ é™¤è¿™ä¸€ç‚¹ï¼Œä¿ç•™ä¸¤ä¸ªæ ¸å¿ƒè´¡çŒ®å³å¯ã€‚
```

### 6.2 ä½ éœ€è¦å®ç°çš„æ ¸å¿ƒä»£ç 

```python
# æ ¸å¿ƒæ–‡ä»¶ï¼šrerank/rank_listwise_os_llm.py

# éœ€è¦æ·»åŠ çš„åŠŸèƒ½ï¼š
# 1. Rubric æ³¨å…¥åˆ° prompt
# 2. æ¨ç†è¾“å‡ºè§£æï¼ˆ<think>...<answer>æ ¼å¼ï¼‰
# 3. åˆ†æ•°è½¬æ¢é€»è¾‘
```

---

## ä¸ƒã€æ€»ç»“

| é—®é¢˜ | ç­”æ¡ˆ |
|------|------|
| **æ‰“åˆ†é€»è¾‘** | ç”Ÿæˆå¼æ¨¡å‹è¾“å‡ºæ’åºé¡ºåº â†’ è½¬æ¢ä¸ºä½ç½®åˆ†æ•° â†’ pytrec_eval è®¡ç®— nDCG |
| **Meta-ranking** | å»ºè®®åˆæœŸåˆ é™¤æˆ–ç®€åŒ–ä¸º fallback æœºåˆ¶ï¼Œèšç„¦æ ¸å¿ƒè´¡çŒ® |
| **æ£€ç´¢å™¨** | åªéœ€æ£€ç´¢ä¸€æ¬¡ï¼Œç»“æœç¼“å­˜åœ¨ `runs/` ç›®å½•ï¼Œè®­ç»ƒ/æ¨ç†å¤ç”¨ |
| **æ¨èæ£€ç´¢å™¨** | BM25ï¼ˆå¿«é€Ÿï¼‰æˆ– ReasonIRï¼ˆSOTAï¼‰ï¼ŒBRIGHT å·²æœ‰ GPT4-CoT å¢å¼ºç»“æœ |
| **æ•°æ®æµ** | æ£€ç´¢(ä¸€æ¬¡) â†’ ç¼“å­˜ â†’ SFTæ•°æ®æ„é€  â†’ DPOæ•°æ®æ„é€  â†’ æ¨ç†è¯„ä¼° |

---

---

## å…«ã€BRIGHT è¯„ä¼°æ¨¡å—

è¯¦ç»†çš„ BRIGHT benchmark è¯„ä¼°å’Œæäº¤æŒ‡å—è¯·å‚è€ƒ:

**ğŸ“ `bright_eval/` ç›®å½•**

```
bright_eval/
â”œâ”€â”€ README.md              # å®Œæ•´çš„è¯„ä¼°å’Œæäº¤æŒ‡å—
â”œâ”€â”€ bright_evaluator.py    # ä¸»è¯„ä¼°è„šæœ¬
â”œâ”€â”€ quick_test.py          # ç¯å¢ƒæ£€æŸ¥è„šæœ¬
â”œâ”€â”€ requirements.txt       # ä¾èµ–åˆ—è¡¨
â””â”€â”€ __init__.py            # Python æ¨¡å—
```

**å¿«é€Ÿä½¿ç”¨:**

```bash
# 1. æ£€æŸ¥ç¯å¢ƒ
cd bright_eval && python quick_test.py

# 2. è¿è¡Œ BM25 åŸºçº¿
python bright_evaluator.py --method bm25 --quick

# 3. ä½¿ç”¨ä½ çš„æ¨¡å‹è¯„ä¼°
python bright_evaluator.py --method custom --reranker_path /path/to/model --all

# 4. ç”Ÿæˆæäº¤æŠ¥å‘Š
python bright_evaluator.py --generate_report
```

---

*æ–‡æ¡£ç‰ˆæœ¬ï¼šv1.1*  
*æ›´æ–°æ—¥æœŸï¼š2026-01-25*
